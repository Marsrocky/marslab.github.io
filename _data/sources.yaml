- id: doi:10.1016/j.patter.2023.100703
  type: paper
  publisher: Patterns, Cell Press
  description: The world-first open-source benchmark on deep learning-empowered WiFi sensing.
  date: 2023-03-10
  image: https://ars.els-cdn.com/content/image/1-s2.0-S2666389923000405-gr3.jpg
  buttons:
    - type: paper
      link: https://www.cell.com/patterns/fulltext/S2666-3899(23)00040-5
    - type: github
      text: Github
      link: xyanchen/WiFi-CSI-Sensing-Benchmark
  tags:
    - wifi sensing
    - benchmark
- id: arxiv:2305.10345
  type: paper
  publisher: NeurIPS 2023 Datasets and Benchmarks Track
  description: The world-first multi-modal non-intrusive 4D human dataset (RGB, Depth, LiDAR, mmWave, WiFi).
  date: 2023-05-11
  image: https://ntu-aiot-lab.github.io/static/images/setup.png
  buttons:
    - type: paper
      link: https://arxiv.org/pdf/2305.10345
    - type: github
      text: Github
      link: ybhbingo/MMFi_dataset
    - type: website
      text: Website
      link: https://ntu-aiot-lab.github.io/mm-fi
  tags:
    - multimodal dataset
    - 4D human sensing
    - human pose estimation
    - non-intrusive sensing
- id: doi:10.1007/s11263-023-01932-5
  type: paper
  image: images/papers/yuecong-ijcv-23.png
  buttons:
    - type: paper
      text: Arxiv
      link: https://arxiv.org/pdf/2202.09545
- id: doi:10.1109/ICCV51070.2023.01724
  type: paper
  image: images/papers/haozhi-mmctta-iccv23.png
  buttons:
    - type: paper
      link: https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_Multi-Modal_Continual_Test-Time_Adaptation_for_3D_Semantic_Segmentation_ICCV_2023_paper.pdf
    - type: website
      text: Website
      link: https://sites.google.com/view/mmcotta
  tags:
    - multimodal learning
    - test-time adaptation
- id: doi:10.48550/arXiv.2305.18712
  publisher: ICLR 2024
  date: 2024-01-01
  image: images/papers/transfer-score.png
  buttons:
    - type: github
      text: Github
      link: sleepyseal/TransferScore
    - type: website
      text: Website
      link: https://sleepyseal.github.io/TransferScoreWeb/
    - type: paper
      link: https://openreview.net/pdf?id=fszrlQ2DuP
  tags:
    - domain adaptation
    - unsupervised evaluation
    - model selection
- id: arXiv:2205.14467
  publisher: ICLR 2023
  image: images/papers/jianfei-beta-iclr23.png
  date: 2023-01-01
  buttons:
    - type: paper
      link: https://arxiv.org/pdf/2205.14467
    - type: github
      text: Github
      link: xyupeng/BETA
  tags:
    - domain adaptation
    - trustworthy AI
- id: doi:10.1109/JIOT.2023.3262940
  publisher: IEEE Internet of Things Journal (IoT-J)
  image: images/gif/metafi.gif
  buttons:
    - type: paper
      link: https://ieeexplore.ieee.org/document/10086600
    - type: github
      text: Github
      link: pridy999/metafi_pose_estimation
  tags:
    - human pose estimation
    - wifi sensing
- id: doi:10.1109/ICRA57147.2024.10610316
  image: images/papers/haozhi-mopa-icra24.gif
  buttons:
    - type: paper
      text: Arxiv
      link: https://arxiv.org/pdf/2309.11839
    - type: github
      text: Github
      link: AronCao49/MoPA
  tags:
    - multimodal learning
    - domain adaptation
    - 3D semantic segmentation
- id: arxiv:2403.06461
  type: paper
  publisher: European Conference on Computer Vision (ECCV), 2024
  image: images/papers/haozhi-latte-eccv24.png
  buttons:
    - type: paper
      link: https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04076.pdf
    - type: github
      text: Github
      link: AronCao49/Latte
    - type: website
      text: Website
      link: https://sites.google.com/view/eccv24-latte
  tags:
    - Multimodal learning
    - Test-time adaptation
- id: doi:10.1145/3679010
  image: images/papers/yuecong-vuda-survey.png
  buttons:
    - type: paper
      link: https://dl.acm.org/doi/10.1145/3679010
    - type: github
      text: Github
      link: xuyu0010/awesome-video-domain-adaptation
  tags:
    - video domain adaptation
    - survey
- id: arXiv:2410.10167
  publisher: ICLR 2025
  date: 2025-01-22
  image: images/papers/xinyan-xfi-iclr25.png
  group: highlighted
  buttons:
    - type: paper
      link: https://doi.org/10.48550/arXiv.2410.10167
    - type: github
      text: Github
      link: xyanchen/X-Fi
    - type: website
      text: Website
      link: https://xyanchen.github.io/X-Fi
  tags:
    - Multimodal learning
    - Foundation model
- id: arxiv:2510.15189
  type: paper
  publisher: ICRA 2026
  description: Real-world RL framework that uses a role-model strategy to label online samples for offline supervised replay, achieving millimeter-level precision in robot manipulation.
  date: 2026-01-22
  image: images/papers/xiangyu-rmrl-icra26.png
  group: highlighted
  buttons:
    - type: paper
      link: https://arxiv.org/pdf/2510.15189
    - type: github
      text: Github
      link: NTUMARS/RMRL
    - type: website
      text: Website
      link: https://ntumars.github.io/project/RMRL/
  tags:
    - reinforcement learning
    - robot manipulation
    - precision
- id: arxiv:2410.02429
  type: paper
  publisher: Patterns, Cell Press
  description: A RAG-enhanced framework for enhancing LLM reasoning from real-world IoT sensor data across diverse tasks.
  date: 2025-10-10
  image: images/papers/tuo-iotllm-cellpatterns25.png
  group: highlighted
  buttons:
    - type: paper
      link: https://www.cell.com/patterns/fulltext/S2666-3899(25)00277-6
    - type: github
      text: Github
      link: NTUMARS/IoT-Agent
  tags:
    - IoT
    - LLM
    - sensor data reasoning
- id: arXiv:2505.10872
  publisher: ICLR 2026
  date: 2026-01-22
  image: images/papers/chenxi-reibench-arxiv25.png
  group: highlighted
  buttons:
    - type: paper
      link: https://doi.org/10.48550/arXiv.2505.10872
  tags:
    - Robot task planning
    - Vagueness
    - LLMs
- id: arXiv:2505.17645 
  publisher: NeurIPS 2025
  date: 2025-05-23
  image: images/papers/chuhao-holollm-arxiv25.png
  group: highlighted
  buttons:
    - type: paper
      link: https://doi.org/10.48550/arXiv.2505.17645
    - type: github
      text: Github
      link: ChuhaoZhou99/HoloLLM
    - type: website
      text: Website
      link: https://chuhaozhou99.github.io/HoloLLM
  tags:
    - Multisensory foundation model
    - Human perception and reasoning
- id: arxiv:2601.20377
  type: paper
  publisher: ICLR 2026
  description: First open-source large-scale wide-band RF dataset and benchmark for fine-grained material identification.
  date: 2026-01-22
  image: images/papers/xinyan-rfmatid-iclr26.png
  buttons:
    - type: paper
      link: https://arxiv.org/pdf/2601.20377
  tags:
    - RF sensing
    - material identification
    - dataset
- id: arxiv:2512.01446
  type: paper
  publisher: CVPR 2026 (Findings)
  description: A photometric re-rendering framework for material-generalized robotic manipulation via computational photography.
  date: 2026-02-23
  image: images/papers/jiayi-m3a-cvpr26.png
  buttons:
    - type: paper
      link: https://arxiv.org/pdf/2512.01446
  tags:
    - robot manipulation
    - material generalization
    - data augmentation
- id: arxiv:2512.04597
  type: paper
  publisher: CVPR 2026
  description: First benchmark for evaluating abstention ability of embodied agents when facing ambiguous human queries.
  date: 2026-02-23
  image: images/papers/tao-abstaineqa-cvpr26.png
  buttons:
    - type: paper
      link: https://arxiv.org/pdf/2512.04597
    - type: website
      text: Website
      link: https://abstaineqa.github.io/
  tags:
    - embodied QA
    - abstention
    - human-robot interaction
- id: arxiv:2512.12378
  type: paper
  publisher: CVPR 2026
  description: The largest multimodal mmWave radar benchmark (661K frames) for high-fidelity human mesh reconstruction.
  date: 2026-02-23
  image: images/papers/junqiao-m4human-cvpr26.png
  buttons:
    - type: paper
      link: https://arxiv.org/pdf/2512.12378
    - type: github
      text: Github
      link: FanJunqiao/M4Human
    - type: website
      text: Website
      link: https://fanjunqiao.github.io/M4Human-site/
  tags:
    - mmWave radar
    - human mesh reconstruction
    - dataset
- id: arxiv:2510.03135
  type: paper
  publisher: AAAI 2026
  description: A two-stage framework for interaction-centric video generation that predicts mask trajectories for both actors and objects.
  date: 2025-11-01
  image: images/papers/gen-mask2iv-aaai26.png
  buttons:
    - type: paper
      link: https://arxiv.org/pdf/2510.03135
    - type: github
      text: Github
      link: Reagan1311/Mask2IV
    - type: website
      text: Website
      link: https://reagan1311.github.io/mask2iv/
  tags:
    - video generation
    - human-object interaction
    - robotic manipulation
- id: arxiv:2511.15379
  type: paper
  publisher: AAAI 2026
  description: A zero-shot framework for open-vocabulary human motion grounding via LLM-guided decomposition and soft masking optimization.
  date: 2025-11-01
  image: images/papers/yunjiao-zomg-aaai26.png
  buttons:
    - type: paper
      link: https://arxiv.org/pdf/2511.15379
    - type: github
      text: Github
      link: pridy999/ZOMG
  tags:
    - motion grounding
    - zero-shot learning
    - test-time training
- id: arxiv:2512.00345
  type: paper
  publisher: AAAI 2026
  description: First diffusion-based framework for radar-based human motion prediction, robust under adverse environments.
  date: 2025-12-01
  image: images/papers/junqiao-mmpred-aaai26.png
  buttons:
    - type: paper
      link: https://arxiv.org/pdf/2512.00345
  tags:
    - mmWave radar
    - human motion prediction
    - diffusion model
