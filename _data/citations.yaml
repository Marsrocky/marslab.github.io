# DO NOT EDIT, GENERATED AUTOMATICALLY

- id: doi:10.1016/j.patter.2023.100703
  title: 'SenseFi: A library and benchmark on deep-learning-empowered WiFi human sensing'
  authors:
  - Jianfei Yang
  - Xinyan Chen
  - Han Zou
  - Chris Xiaoxuan Lu
  - Dazhuo Wang
  - Sumei Sun
  - Lihua Xie
  publisher: Patterns, Cell Press
  date: '2023-03-10'
  link: https://doi.org/gsmbtr
  type: paper
  description: The world-first open-source benchmark on deep learning-empowered WiFi
    sensing.
  image: https://ars.els-cdn.com/content/image/1-s2.0-S2666389923000405-gr3.jpg
  buttons:
  - type: paper
    link: https://www.cell.com/patterns/fulltext/S2666-3899(23)00040-5
  - type: github
    text: Github
    link: xyanchen/WiFi-CSI-Sensing-Benchmark
  tags:
  - wifi sensing
  - benchmark
  plugin: sources.py
  file: sources.yaml
- id: arxiv:2305.10345
  title: 'MM-Fi: Multi-Modal Non-Intrusive 4D Human Dataset for Versatile Wireless
    Sensing'
  authors:
  - Jianfei Yang
  - He Huang
  - Yunjiao Zhou
  - Xinyan Chen
  - Yuecong Xu
  - Shenghai Yuan
  - Han Zou
  - Chris Xiaoxuan Lu
  - Lihua Xie
  publisher: NeurIPS 2023 Datasets and Benchmarks Track
  date: '2023-05-11'
  link: https://arxiv.org/abs/2305.10345
  type: paper
  description: The world-first multi-modal non-intrusive 4D human dataset (RGB, Depth,
    LiDAR, mmWave, WiFi).
  image: https://ntu-aiot-lab.github.io/static/images/setup.png
  buttons:
  - type: paper
    link: https://arxiv.org/pdf/2305.10345
  - type: github
    text: Github
    link: ybhbingo/MMFi_dataset
  - type: website
    text: Website
    link: https://ntu-aiot-lab.github.io/mm-fi
  tags:
  - multimodal dataset
  - 4D human sensing
  - human pose estimation
  - non-intrusive sensing
  plugin: sources.py
  file: sources.yaml
- id: doi:10.1007/s11263-023-01932-5
  title: 'Going Deeper into Recognizing Actions in Dark Environments: A Comprehensive
    Benchmark Study'
  authors:
  - Yuecong Xu
  - Haozhi Cao
  - Jianxiong Yin
  - Zhenghua Chen
  - Xiaoli Li
  - Zhengguo Li
  - Qianwen Xu
  - Jianfei Yang
  publisher: International Journal of Computer Vision
  date: '2023-11-08'
  link: https://doi.org/g8pwz3
  type: paper
  image: images/papers/yuecong-ijcv-23.png
  buttons:
  - type: paper
    text: Arxiv
    link: https://arxiv.org/pdf/2202.09545
  plugin: sources.py
  file: sources.yaml
- id: doi:10.1109/ICCV51070.2023.01724
  title: Multi-Modal Continual Test-Time Adaptation for 3D Semantic Segmentation
  authors:
  - Haozhi Cao
  - Yuecong Xu
  - Jianfei Yang
  - Pengyu Yin
  - Shenghai Yuan
  - Lihua Xie
  publisher: 2023 IEEE/CVF International Conference on Computer Vision (ICCV)
  date: '2023-10-01'
  link: https://doi.org/g8pxwc
  type: paper
  image: images/papers/haozhi-mmctta-iccv23.png
  buttons:
  - type: paper
    link: https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_Multi-Modal_Continual_Test-Time_Adaptation_for_3D_Semantic_Segmentation_ICCV_2023_paper.pdf
  - type: website
    text: Website
    link: https://sites.google.com/view/mmcotta
  tags:
  - multimodal learning
  - test-time adaptation
  plugin: sources.py
  file: sources.yaml
- id: doi:10.48550/arXiv.2305.18712
  title: Can We Evaluate Domain Adaptation Models Without Target-Domain Labels?
  authors:
  - Jianfei Yang
  - Hanjie Qian
  - Yuecong Xu
  - Kai Wang
  - Lihua Xie
  publisher: ICLR 2024
  date: '2024-01-01'
  link: https://doi.org/g8pxwd
  image: images/papers/transfer-score.png
  buttons:
  - type: github
    text: Github
    link: sleepyseal/TransferScore
  - type: website
    text: Website
    link: https://sleepyseal.github.io/TransferScoreWeb/
  - type: paper
    link: https://openreview.net/pdf?id=fszrlQ2DuP
  tags:
  - domain adaptation
  - unsupervised evaluation
  - model selection
  plugin: sources.py
  file: sources.yaml
- id: arXiv:2205.14467
  title: 'Divide to Adapt: Mitigating Confirmation Bias for Domain Adaptation of Black-Box
    Predictors'
  authors:
  - Jianfei Yang
  - Xiangyu Peng
  - Kai Wang
  - Zheng Zhu
  - Jiashi Feng
  - Lihua Xie
  - Yang You
  publisher: ICLR 2023
  date: '2023-01-01'
  link: https://arxiv.org/abs/2205.14467
  image: images/papers/jianfei-beta-iclr23.png
  buttons:
  - type: paper
    link: https://arxiv.org/pdf/2205.14467
  - type: github
    text: Github
    link: xyupeng/BETA
  tags:
  - domain adaptation
  - trustworthy AI
  plugin: sources.py
  file: sources.yaml
- id: doi:10.1109/JIOT.2023.3262940
  title: 'MetaFi++: WiFi-Enabled Transformer-Based Human Pose Estimation for Metaverse
    Avatar Simulation'
  authors:
  - Yunjiao Zhou
  - He Huang
  - Shenghai Yuan
  - Han Zou
  - Lihua Xie
  - Jianfei Yang
  publisher: IEEE Internet of Things Journal (IoT-J)
  date: '2023-08-15'
  link: https://doi.org/g8qc4m
  image: images/gif/metafi.gif
  buttons:
  - type: paper
    link: https://ieeexplore.ieee.org/document/10086600
  - type: github
    text: Github
    link: pridy999/metafi_pose_estimation
  tags:
  - human pose estimation
  - wifi sensing
  plugin: sources.py
  file: sources.yaml
- id: doi:10.1109/ICRA57147.2024.10610316
  title: 'MoPA: Multi-Modal Prior Aided Domain Adaptation for 3D Semantic Segmentation'
  authors:
  - Haozhi Cao
  - Yuecong Xu
  - Jianfei Yang
  - Pengyu Yin
  - Shenghai Yuan
  - Lihua Xie
  publisher: 2024 IEEE International Conference on Robotics and Automation (ICRA)
  date: '2024-05-13'
  link: https://doi.org/g8qdjg
  image: images/papers/haozhi-mopa-icra24.gif
  buttons:
  - type: paper
    text: Arxiv
    link: https://arxiv.org/pdf/2309.11839
  - type: github
    text: Github
    link: AronCao49/MoPA
  tags:
  - multimodal learning
  - domain adaptation
  - 3D semantic segmentation
  plugin: sources.py
  file: sources.yaml
- id: arxiv:2403.06461
  title: Interactive Test-Time Adaptation with Reliable Spatial-Temporal Voxels for
    Multi-Modal Segmentation
  authors:
  - Haozhi Cao
  - Yuecong Xu
  - Pengyu Yin
  - Xingyu Ji
  - Shenghai Yuan
  - Jianfei Yang
  - Lihua Xie
  publisher: European Conference on Computer Vision (ECCV), 2024
  date: '2025-10-07'
  link: https://arxiv.org/abs/2403.06461
  type: paper
  image: images/papers/haozhi-latte-eccv24.png
  buttons:
  - type: paper
    link: https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04076.pdf
  - type: github
    text: Github
    link: AronCao49/Latte
  - type: website
    text: Website
    link: https://sites.google.com/view/eccv24-latte
  tags:
  - Multimodal learning
  - Test-time adaptation
  plugin: sources.py
  file: sources.yaml
- id: doi:10.1145/3679010
  title: 'Video Unsupervised Domain Adaptation with Deep Learning: A Comprehensive
    Survey'
  authors:
  - Yuecong Xu
  - Haozhi Cao
  - Lihua Xie
  - Xiao-li Li
  - Zhenghua Chen
  - Jianfei Yang
  publisher: ACM Computing Surveys
  date: '2024-10-01'
  link: https://doi.org/g8pwzk
  image: images/papers/yuecong-vuda-survey.png
  buttons:
  - type: paper
    link: https://dl.acm.org/doi/10.1145/3679010
  - type: github
    text: Github
    link: xuyu0010/awesome-video-domain-adaptation
  tags:
  - video domain adaptation
  - survey
  plugin: sources.py
  file: sources.yaml
- id: arXiv:2410.10167
  title: 'X-Fi: A Modality-Invariant Foundation Model for Multimodal Human Sensing'
  authors:
  - Xinyan Chen
  - Jianfei Yang
  publisher: ICLR 2025
  date: '2025-01-22'
  link: https://arxiv.org/abs/2410.10167
  image: images/papers/xinyan-xfi-iclr25.png
  group: highlighted
  buttons:
  - type: paper
    link: https://doi.org/10.48550/arXiv.2410.10167
  - type: github
    text: Github
    link: xyanchen/X-Fi
  - type: website
    text: Website
    link: https://xyanchen.github.io/X-Fi
  tags:
  - Multimodal learning
  - Foundation model
  plugin: sources.py
  file: sources.yaml
- id: arxiv:2510.15189
  title: 'RM-RL: Role-Model Reinforcement Learning for Precise Robot Manipulation'
  authors:
  - Xiangyu Chen
  - Chuhao Zhou
  - Yuxi Liu
  - Jianfei Yang
  publisher: ICRA 2026
  date: '2026-01-22'
  link: https://arxiv.org/abs/2510.15189
  type: paper
  description: Real-world RL framework that uses a role-model strategy to label online
    samples for offline supervised replay, achieving millimeter-level precision in
    robot manipulation.
  image: images/papers/xiangyu-rmrl-icra26.png
  group: highlighted
  buttons:
  - type: paper
    link: https://arxiv.org/pdf/2510.15189
  - type: github
    text: Github
    link: NTUMARS/RMRL
  - type: website
    text: Website
    link: https://ntumars.github.io/project/RMRL/
  tags:
  - reinforcement learning
  - robot manipulation
  - precision
  plugin: sources.py
  file: sources.yaml
- id: arxiv:2410.02429
  title: 'IoT-LLM: A framework for enhancing large language model reasoning from
    real-world sensor data'
  authors:
  - Tuo An
  - Yunjiao Zhou
  - Han Zou
  - Jianfei Yang
  publisher: Patterns, Cell Press
  date: '2025-10-10'
  link: https://arxiv.org/abs/2410.02429
  type: paper
  description: A RAG-enhanced framework for enhancing LLM reasoning from real-world
    IoT sensor data across diverse tasks.
  image: images/papers/tuo-iotllm-cellpatterns25.png
  group: highlighted
  buttons:
  - type: paper
    link: https://www.cell.com/patterns/fulltext/S2666-3899(25)00277-6
  - type: github
    text: Github
    link: NTUMARS/IoT-Agent
  tags:
  - IoT
  - LLM
  - sensor data reasoning
  plugin: sources.py
  file: sources.yaml
- id: arXiv:2505.10872
  title: 'REI-Bench: Can Embodied Agents Understand Vague Human Instructions in Task
    Planning?'
  authors:
  - Chenxi Jiang
  - Chuhao Zhou
  - Jianfei Yang
  publisher: ICLR 2026
  date: '2026-01-22'
  link: https://arxiv.org/abs/2505.10872
  image: images/papers/chenxi-reibench-arxiv25.png
  group: highlighted
  buttons:
  - type: paper
    link: https://doi.org/10.48550/arXiv.2505.10872
  tags:
  - Robot task planning
  - Vagueness
  - LLMs
  plugin: sources.py
  file: sources.yaml
- id: arXiv:2505.17645
  title: 'HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing
    and Reasoning'
  authors:
  - Chuhao Zhou
  - Jianfei Yang
  publisher: NeurIPS 2025
  date: '2025-05-23'
  link: https://arxiv.org/abs/2505.17645
  image: images/papers/chuhao-holollm-arxiv25.png
  group: highlighted
  buttons:
  - type: paper
    link: https://doi.org/10.48550/arXiv.2505.17645
  - type: github
    text: Github
    link: ChuhaoZhou99/HoloLLM
  - type: website
    text: Website
    link: https://chuhaozhou99.github.io/HoloLLM
  tags:
  - Multisensory foundation model
  - Human perception and reasoning
  plugin: sources.py
  file: sources.yaml
- id: arxiv:2601.20377
  title: 'RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification'
  authors:
  - Xinyan Chen
  - Qinchun Li
  - Ruiqin Ma
  - Jiaqi Bai
  - Li Yi
  - Jianfei Yang
  publisher: ICLR 2026
  date: '2026-01-22'
  link: https://arxiv.org/abs/2601.20377
  type: paper
  description: First open-source large-scale wide-band RF dataset and benchmark for
    fine-grained material identification.
  image: images/papers/xinyan-rfmatid-iclr26.png
  buttons:
  - type: paper
    link: https://arxiv.org/pdf/2601.20377
  tags:
  - RF sensing
  - material identification
  - dataset
  plugin: sources.py
  file: sources.yaml
- id: arxiv:2512.01446
  title: 'M3A Policy: Mutable Material Manipulation Augmentation Policy through Photometric
    Re-rendering'
  authors:
  - Jiayi Li
  - Yuxuan Hu
  - Haoran Geng
  - Xiangyu Chen
  - Chuhao Zhou
  - Ziteng Cui
  - Jianfei Yang
  publisher: CVPR 2026 (Findings)
  date: '2026-02-23'
  link: https://arxiv.org/abs/2512.01446
  type: paper
  description: A photometric re-rendering framework for material-generalized robotic
    manipulation via computational photography.
  image: images/papers/jiayi-m3a-cvpr26.png
  buttons:
  - type: paper
    link: https://arxiv.org/pdf/2512.01446
  tags:
  - robot manipulation
  - material generalization
  - data augmentation
  plugin: sources.py
  file: sources.yaml
- id: arxiv:2512.04597
  title: "When Robots Should Say \u201CI Don\u2019t Know\u201D: Benchmarking Abstention\
    \ in Embodied Question Answering"
  authors:
  - Tao Wu
  - Chuhao Zhou
  - Guangyu Zhao
  - Haozhi Cao
  - Yewen Pu
  - Jianfei Yang
  publisher: CVPR 2026
  date: '2026-02-23'
  link: https://arxiv.org/abs/2512.04597
  type: paper
  description: First benchmark for evaluating abstention ability of embodied agents
    when facing ambiguous human queries.
  image: images/papers/tao-abstaineqa-cvpr26.png
  buttons:
  - type: paper
    link: https://arxiv.org/pdf/2512.04597
  - type: website
    text: Website
    link: https://abstaineqa.github.io/
  tags:
  - embodied QA
  - abstention
  - human-robot interaction
  plugin: sources.py
  file: sources.yaml
- id: arxiv:2512.12378
  title: 'M4Human: A Large-Scale Multimodal mmWave Radar Benchmark for Human Mesh
    Reconstruction'
  authors:
  - Junqiao Fan
  - Yunjiao Zhou
  - Yizhuo Yang
  - Xinyuan Cui
  - Jiarui Zhang
  - Lihua Xie
  - Jianfei Yang
  - Chris Xiaoxuan Lu
  - Fangqiang Ding
  publisher: CVPR 2026
  date: '2026-02-23'
  link: https://arxiv.org/abs/2512.12378
  type: paper
  description: The largest multimodal mmWave radar benchmark (661K frames) for high-fidelity
    human mesh reconstruction.
  image: images/papers/junqiao-m4human-cvpr26.png
  buttons:
  - type: paper
    link: https://arxiv.org/pdf/2512.12378
  - type: github
    text: Github
    link: FanJunqiao/M4Human
  - type: website
    text: Website
    link: https://fanjunqiao.github.io/M4Human-site/
  tags:
  - mmWave radar
  - human mesh reconstruction
  - dataset
  plugin: sources.py
  file: sources.yaml
- id: arxiv:2510.03135
  title: 'Mask2IV: Interaction-Centric Video Generation via Mask Trajectories'
  authors:
  - Gen Li
  - Bo Zhao
  - Jianfei Yang
  - Laura Sevilla-Lara
  publisher: AAAI 2026
  date: '2025-11-01'
  link: https://arxiv.org/abs/2510.03135
  type: paper
  description: A two-stage framework for interaction-centric video generation that
    predicts mask trajectories for both actors and objects.
  image: images/papers/gen-mask2iv-aaai26.png
  buttons:
  - type: paper
    link: https://arxiv.org/pdf/2510.03135
  - type: github
    text: Github
    link: Reagan1311/Mask2IV
  - type: website
    text: Website
    link: https://reagan1311.github.io/mask2iv/
  tags:
  - video generation
  - human-object interaction
  - robotic manipulation
  plugin: sources.py
  file: sources.yaml
- id: arxiv:2511.15379
  title: 'ZOMG: Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training'
  authors:
  - Yunjiao Zhou
  - Xinyan Chen
  - Junlang Qian
  - Lihua Xie
  - Jianfei Yang
  publisher: AAAI 2026
  date: '2025-11-01'
  link: https://arxiv.org/abs/2511.15379
  type: paper
  description: A zero-shot framework for open-vocabulary human motion grounding via
    LLM-guided decomposition and soft masking optimization.
  image: images/papers/yunjiao-zomg-aaai26.png
  buttons:
  - type: paper
    link: https://arxiv.org/pdf/2511.15379
  - type: github
    text: Github
    link: pridy999/ZOMG
  tags:
  - motion grounding
  - zero-shot learning
  - test-time training
  plugin: sources.py
  file: sources.yaml
- id: arxiv:2512.00345
  title: 'mmPred: Radar-based Human Motion Prediction in the Dark'
  authors:
  - Junqiao Fan
  - Haocong Rao
  - Jiarui Zhang
  - Jianfei Yang
  - Lihua Xie
  publisher: AAAI 2026
  date: '2025-12-01'
  link: https://arxiv.org/abs/2512.00345
  type: paper
  description: First diffusion-based framework for radar-based human motion prediction,
    robust under adverse environments.
  image: images/papers/junqiao-mmpred-aaai26.png
  buttons:
  - type: paper
    link: https://arxiv.org/pdf/2512.00345
  tags:
  - mmWave radar
  - human motion prediction
  - diffusion model
  plugin: sources.py
  file: sources.yaml
